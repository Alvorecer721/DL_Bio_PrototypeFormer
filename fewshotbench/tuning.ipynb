{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "import wandb\n",
    "import torch\n",
    "import time\n",
    "from omegaconf import OmegaConf\n",
    "from utils.io_utils import get_resume_file, hydra_setup, fix_seed, model_to_dict, opt_to_dict, get_model_file\n",
    "from run import train, test\n",
    "from prettytable import PrettyTable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hydra context manager\n",
    "    with initialize(config_path=\"conf\"):\n",
    "        # Compose the configuration\n",
    "        cfg = compose(config_name=\"main\", overrides=[\n",
    "            f\"exp.name=optuna_trial_{trial.number}\",\n",
    "            f\"optimizer_cls.lr={trial.suggest_loguniform('lr', 1e-7, 1e-4)}\",\n",
    "        ])\n",
    "\n",
    "        # Print the configuration for debugging\n",
    "        print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "        # # Your existing model initialization and training logic\n",
    "        # train_loader, val_loader, model = initialize_dataset_model(cfg)\n",
    "        # model = train(train_loader, val_loader, model, cfg)\n",
    "\n",
    "        # # Evaluate the model and return the metric you are interested in\n",
    "        # acc_mean, _ = test(cfg, model, 'val')  # Assuming 'val' is your validation split\n",
    "        # return acc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  type: classification\n",
      "  simple_cls:\n",
      "    _target_: datasets.prot.swissprot.SPSimpleDataset\n",
      "  set_cls:\n",
      "    n_way: ${n_way}\n",
      "    n_support: ${n_shot}\n",
      "    n_query: ${n_query}\n",
      "    _target_: datasets.prot.swissprot.SPSetDataset\n",
      "    embed_dir: ${dataset.embed_dir}\n",
      "  name: swissprot_no_bacbone\n",
      "  embed_dir: embeds\n",
      "eval_split:\n",
      "- train\n",
      "- val\n",
      "- test\n",
      "backbone:\n",
      "  _target_: backbones.id.Id\n",
      "train_classes: 59\n",
      "n_way: 5\n",
      "n_shot: 5\n",
      "n_query: 15\n",
      "method:\n",
      "  name: protoformer\n",
      "  train_batch: null\n",
      "  val_batch: null\n",
      "  fast_weight: false\n",
      "  start_epoch: 0\n",
      "  eval_type: set\n",
      "  stop_epoch: 60\n",
      "  type: meta\n",
      "  cls:\n",
      "    n_way: ${n_way}\n",
      "    n_support: ${n_shot}\n",
      "    _target_: methods.protoformer.ProtoFormer\n",
      "    n_layer: 2\n",
      "    n_head: 2\n",
      "    contrastive_coef: 1\n",
      "    n_sub_support: 5\n",
      "    ffn_dim: 512\n",
      "    dropout: 0.1\n",
      "    norm_first: false\n",
      "    contrastive_loss: original\n",
      "model: protoformer\n",
      "mode: train\n",
      "exp:\n",
      "  name: optuna_trial_1\n",
      "  save_freq: 10\n",
      "  resume: false\n",
      "  seed: 42\n",
      "  val_freq: 1\n",
      "optimizer: Adam\n",
      "lr: 1.0e-05\n",
      "weight_decay: 0.01\n",
      "optimizer_cls:\n",
      "  _target_: torch.optim.${optimizer}\n",
      "  lr: ${lr}\n",
      "  weight_decay: ${weight_decay}\n",
      "checkpoint:\n",
      "  dir: checkpoints/${exp.name}/${dataset.name}/${method.name}_${model}\n",
      "  test_iter: best_model\n",
      "  time: latest\n",
      "wandb:\n",
      "  project: fewshotbench\n",
      "  entity: cs502\n",
      "  mode: online\n",
      "iter_num: 600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/gbdx6wws5kb_m82h5gt5pf440000gn/T/ipykernel_1242/1225602387.py:2: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"conf\"):\n"
     ]
    }
   ],
   "source": [
    "# Hydra context manager\n",
    "with initialize(config_path=\"conf\"):\n",
    "    dataset = 'swissprot_no_backbone' \n",
    "    method = 'protoformer'\n",
    "    num_sub_support = 5\n",
    "    n_layer = 2\n",
    "    n_head = 2\n",
    "    contrastive_coef = 1\n",
    "    norm_first = False\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.1\n",
    "    contrastive_loss = 'original'\n",
    "\n",
    "    # Compose the configuration with overrides\n",
    "    cfg = compose(config_name=\"main\", overrides=[\n",
    "        f\"model={method}\", \n",
    "        f\"method={method}\",\n",
    "        f\"dataset={dataset}\",  # Example dataset\n",
    "        f\"lr={0.00001}\",  # Example learning rate\n",
    "        f\"weight_decay={weight_decay}\",\n",
    "        f\"method.cls.n_sub_support={num_sub_support}\",\n",
    "        f\"method.cls.n_layer={n_layer}\",\n",
    "        f\"method.cls.n_head={n_head}\",\n",
    "        f\"method.cls.contrastive_coef={contrastive_coef}\",\n",
    "        f\"method.cls.dropout={dropout}\",\n",
    "        f\"method.cls.norm_first={norm_first}\",\n",
    "        f\"method.cls.contrastive_loss={contrastive_loss}\",\n",
    "        f\"exp.name=optuna_trial_{1}\",\n",
    "        f\"method.cls.ffn_dim={512}\",\n",
    "        # Add more overrides if necessary\n",
    "    ])\n",
    "\n",
    "    # Print the configuration for debugging\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(dataset=\"swissprot_no_backbone\"):\n",
    "\n",
    "    with initialize(config_path=\"conf/dataset\", version_base=None):\n",
    "        cfg = compose(config_name=dataset)\n",
    "        \n",
    "    train_dataset = instantiate(cfg.dataset.set_cls, mode='train')\n",
    "    val_dataset = instantiate(cfg.dataset.set_cls, mode='val')\n",
    "\n",
    "    train_loader = train_dataset.get_data_loader()\n",
    "    val_loader = val_dataset.get_data_loader()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def objective(trial):\n",
    "        with initialize(config_path=\"conf\"):\n",
    "            # Compose the configuration with trial-specific overrides\n",
    "            cfg = compose(config_name=\"main\", overrides=[\n",
    "                \"model=protoformer\",\n",
    "                \"method=protoformer\",\n",
    "                f\"dataset={dataset}\",  # Fixed dataset\n",
    "                f\"optimizer_cls.lr={trial.suggest_float('lr', 1e-7, 1e-4)}\",\n",
    "                f\"optimizer_cls.weight_decay={trial.suggest_float('weight_decay', 1e-5, 1e-3)}\",\n",
    "                f\"method.cls.n_sub_support={trial.suggest_int('n_sub_support', 2, 4)}\",\n",
    "                f\"method.cls.n_layer={trial.suggest_int('n_layer', 1, 3)}\",\n",
    "                f\"method.cls.n_head={trial.suggest_categorical('n_head', [1, 2, 3, 4, 5, 8])}\",\n",
    "                f\"method.cls.contrastive_coef={trial.suggest_float('contrastive_coef', 0.1, 2.0)}\",\n",
    "                f\"method.cls.dropout={trial.suggest_float('dropout', 0.0, 0.5)}\",\n",
    "                f\"method.cls.norm_first={trial.suggest_categorical('norm_first', [True, False])}\",\n",
    "                f\"method.cls.contrastive_loss=original\", #TODO: you can modify this\n",
    "                f\"exp.name=optuna_trial_{trial.number}\",\n",
    "            ])\n",
    "\n",
    "            fix_seed(cfg.exp.seed)\n",
    "\n",
    "            print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "            # Initialize model and backbone for this trial\n",
    "            backbone = instantiate(cfg.backbone, x_dim=train_dataset.dim)\n",
    "            model = instantiate(cfg.method.cls, backbone=backbone)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "\n",
    "            model = train(train_loader, val_loader, model, cfg)\n",
    "\n",
    "            acc_mean, acc_std = test(cfg, model, split='val')\n",
    "\n",
    "            results.append([trial.number, acc_mean, acc_std])\n",
    "\n",
    "            return acc_mean  # or any other metric you want to optimize\n",
    "\n",
    "    # Run Optuna study\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=2)\n",
    "\n",
    "    # Output the optimization results\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial: {best_trial.number}\")\n",
    "    print(f\"Best Value: {best_trial.value}\")\n",
    "    print(f\"Best Parameters: {best_trial.params}\")\n",
    "\n",
    "    # Save the study\n",
    "    optuna_studies_file = f\"{dataset}_studies.pkl\"\n",
    "    with open(optuna_studies_file, \"wb\") as f:\n",
    "        pickle.dump(study, f)\n",
    "\n",
    "    # Log results to WandB\n",
    "    table = wandb.Table(data=results, columns=[\"trial\", \"acc_mean\", \"acc_std\"])\n",
    "    wandb.log({\"eval_results\": table})\n",
    "\n",
    "    # Display results in a pretty table\n",
    "    display_table = PrettyTable([\"trial\", \"acc_mean\", \"acc_std\"])\n",
    "    for row in results:\n",
    "        display_table.add_row(row)\n",
    "    print(display_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with initialize(config_path=\"conf/dataset\", version_base=None):\n",
    "#     cfg = compose(config_name=\"swissprot_no_backbone\")\n",
    "#     print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2023",
   "language": "python",
   "name": "ml2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
